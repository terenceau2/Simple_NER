{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "This notebook is a tutorial of how to do named entity recognition (NER) using the CoNLL-2003 baseline model. \n",
    "In the baseline model, the training involves creating a \"dictionary\" that records all the named entities encountered in the train dataset.\n",
    "\n",
    "During inference, a phrase is tagged if it appears in the \"dictionary\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions for data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename,delimiter=','):\n",
    "    \n",
    "    with open(filename) as myfile:\n",
    "            data = myfile.readlines()\n",
    "            data = [i.rstrip('\\n') for i in data]\n",
    "\n",
    "    data = [i.split(delimiter) for i in data]\n",
    "\n",
    "\n",
    "\n",
    "    for i in data:\n",
    "                if i != [''] and i!=[]:\n",
    "                    del i[1]\n",
    "                    del i[1]  # delete the middle 2 columns from the data\n",
    "    for i in range(0, len(data)):\n",
    "                if data[i] == [''] or data[i]==[]:\n",
    "                    data[i] = [\"\", \"O\"]\n",
    "\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2sents(data_string_list):\n",
    "    data = list()\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for data_string in data_string_list:\n",
    "\n",
    "        if data_string == ['', 'O'] or data_string == ['']:\n",
    "            if X == ['-DOCSTART-']:\n",
    "                X = list()\n",
    "                Y = list()\n",
    "                continue\n",
    "\n",
    "            data.append((X, Y))\n",
    "            X = list()\n",
    "            Y = list()\n",
    "        else:\n",
    "\n",
    "            X.append(data_string[0])\n",
    "            Y.append(data_string[-1])\n",
    "\n",
    "    if len(X) > 0:\n",
    "        data.append((X, Y))\n",
    "\n",
    "    data = [x for x in data if len(x) != 0]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tags_space=['B-PER', 'B-LOC', 'B-ORG','B-MISC', 'I-PER', 'I-LOC', 'I-ORG', 'I-MISC', 'O']\n",
    "\n",
    "\"\"\"\n",
    "the data should be in CoNLL-2003 dataset's format.\n",
    "each line looks like this:\n",
    "(this tutorial does not provide the CoNLL-2003 annotated dataset)\n",
    "\n",
    "-DOCSTART- -X- O O\n",
    "\n",
    "EU NNP I-NP I-ORG\n",
    "rejects VBZ I-VP O\n",
    "German JJ I-NP I-MISC\n",
    "call NN I-NP O\n",
    "to TO I-VP O\n",
    "boycott VB I-VP O\n",
    "British JJ I-NP I-MISC\n",
    "lamb NN I-NP O\n",
    ". . O O\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_path= 'your_path' #you need to put train path here\n",
    "test_path='your_path' #you need to put test path here\n",
    "\n",
    "train=preprocess(train_path, delimiter=' ')\n",
    "test=preprocess(test_path, delimiter=' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set=word2sents(train)\n",
    "\n",
    "\n",
    "test_set=word2sents(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_truth=[x[1] for x in test_set]\n",
    "test_tokens=[x[0] for x in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentize(lis):\n",
    "    #for a segment, the end is inclusive, ie, (3,4) means 3rd and 4th token forms a segment\n",
    "\n",
    "    segs=[]\n",
    "\n",
    "    currenttag=lis[0]\n",
    "    buffer=0\n",
    "\n",
    "    for i in range(1,len(lis)):\n",
    "        if lis[i]!=currenttag:\n",
    "            if currenttag!='O' and len(currenttag)>1:\n",
    "                currenttag=currenttag[2:]\n",
    "            segs.append((buffer,i-1,currenttag))\n",
    "            currenttag=lis[i]\n",
    "            if lis[i].startswith('B-'):\n",
    "                currenttag=lis[i].replace('B-','I-')\n",
    "            buffer=i\n",
    "\n",
    "    if currenttag != 'O' and len(currenttag)>1:\n",
    "        currenttag = currenttag[2:]\n",
    "    segs.append((buffer,len(lis)-1,currenttag))\n",
    "\n",
    "    return segs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_class_priority(entity_class):\n",
    "    # Define the priority for the classes (higher values mean higher priority)\n",
    "    class_priority = {\n",
    "        \"LOC\": 4,\n",
    "        \"ORG\": 3,\n",
    "        \"PER\": 2,\n",
    "        \"MISC\": 1\n",
    "    }\n",
    "    # Return the priority of the class, default to 0 if class is not found\n",
    "    return class_priority.get(entity_class, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "entity_dictionary={}\n",
    "\n",
    "\n",
    "for i in range(0,len(train_set)):\n",
    "    sentence=train_set[i]\n",
    "    tokens=sentence[0]\n",
    "    tags=sentence[1]\n",
    "    tags=segmentize(tags)\n",
    "    tags=[x for x in tags if x[2]!='O']\n",
    "\n",
    "    for ent in tags:\n",
    "        phrase=tokens[ent[0]:ent[1]+1]\n",
    "        phrase=' '.join(phrase)\n",
    "        ent_class=ent[2]\n",
    "\n",
    "\n",
    "        if phrase not in entity_dictionary:\n",
    "            # If the phrase doesn't exist, initialize an empty dictionary for class counters\n",
    "            entity_dictionary[phrase] = {}\n",
    "\n",
    "        # If the class already exists for this phrase, increment the counter\n",
    "        if ent_class in entity_dictionary[phrase]:\n",
    "            entity_dictionary[phrase][ent_class] += 1\n",
    "        else:\n",
    "            # Otherwise, initialize the counter for this class\n",
    "            entity_dictionary[phrase][ent_class] = 1\n",
    "\n",
    "\n",
    "entity_dictionary = {key: entity_dictionary[key] for key in sorted(entity_dictionary)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for phrase, classes in entity_dictionary.items():\n",
    "    # If the phrase has more than one class, find the one with the highest counter\n",
    "    if len(classes) > 1:\n",
    "        # Sort classes by counter and select the class with the highest count\n",
    "        max_count = max(classes.values())\n",
    "\n",
    "        # Step 2: Filter classes that have the highest count\n",
    "        candidate_classes = [cls for cls, count in classes.items() if count == max_count]\n",
    "\n",
    "        # Step 3: Apply tiebreaking rule to choose the class with the highest priority. See the \"get_class_priority\" function. You can customize it according to your preference\n",
    "        best_class = max(candidate_classes, key=get_class_priority)\n",
    "\n",
    "        # Keep only the class with the highest counter\n",
    "        entity_dictionary[phrase] = {best_class: classes[best_class]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_tag(test_sentence, entity_dictionary, minoccur=0 ): \n",
    "\n",
    "\n",
    "    sentence_length = len(test_sentence)\n",
    "\n",
    "    prefixes = set()\n",
    "    for key in entity_dictionary:\n",
    "        key_tokens = key.split()\n",
    "        for i in range(1, len(key_tokens) + 1):\n",
    "            prefixes.add(' '.join(key_tokens[:i]))\n",
    "\n",
    "    prefixes=sorted(list(prefixes))\n",
    "\n",
    "\n",
    "    found_entities=[]\n",
    "\n",
    "    i = 0\n",
    "    while i <  sentence_length:\n",
    "        buffer = test_sentence[i]  # Start the buffer with the current token\n",
    "        buffer_str = buffer\n",
    "\n",
    "        j = i + 1  # Pointer to track subsequent tokens\n",
    "        last_match = None  # To store the longest valid match\n",
    "\n",
    "        # Keep expanding the buffer while it matches any dictionary key\n",
    "        while j <= sentence_length:\n",
    "            # Join the tokens in the buffer and check against the entity dictionary\n",
    "            if buffer_str in prefixes:\n",
    "                # Check if buffer_str is a valid entity in the dictionary\n",
    "                if buffer_str in entity_dictionary:\n",
    "                    ent_class, counter = list(entity_dictionary[buffer_str].items())[0]\n",
    "\n",
    "                    last_match = (buffer_str, (i, j-1), ent_class, counter)\n",
    "\n",
    "                # If more tokens are available, add them to buffer_str\n",
    "                if j < sentence_length:\n",
    "                    buffer_str += ' ' + test_sentence[j]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "            j += 1  # Expand the buffer by adding one more token\n",
    "\n",
    "        # If we found a match, append the longest match found to the result list\n",
    "        if last_match is not None:\n",
    "            found_entities.append(last_match)\n",
    "\n",
    "        if last_match is not None:\n",
    "            i = j-1  # Move the index to the position after the matched entity\n",
    "        else:\n",
    "            i = i + 1  # Move to the next token if no match was found\n",
    "\n",
    "    found_entities= [x for x in found_entities if x[3]>=minoccur]\n",
    "    found_entities = sorted(found_entities, key=lambda x: x[1][0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    predlist = [\"O\"] * sentence_length\n",
    "\n",
    "    for entity, (start_idx, end_idx), entity_class, _ in found_entities:\n",
    "        if start_idx > 0 and predlist[start_idx - 1].endswith(entity_class):\n",
    "            # Mark the first token with \"B-\" if the previous token is part of the same class\n",
    "            predlist[start_idx] = f\"B-{entity_class}\"\n",
    "        else:\n",
    "            # Otherwise, mark the first token with \"I-\"\n",
    "            predlist[start_idx] = f\"I-{entity_class}\"\n",
    "\n",
    "        # Mark the rest of the tokens in the entity with \"I-\"\n",
    "        for i in range(start_idx + 1, end_idx + 1):\n",
    "            predlist[i] = f\"I-{entity_class}\"\n",
    "\n",
    "    return predlist\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "for i in range(0,len(test_tokens)):\n",
    "\n",
    "    test_sentence= test_tokens[i]\n",
    "\n",
    "    predlist=baseline_tag(test_sentence,entity_dictionary)\n",
    "\n",
    "    predictions.append(predlist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def endofphrase(prev, current):#if the previous word is the last word of a NE phrase, then returns true\n",
    "    answer=False\n",
    "    if prev.startswith(\"B\") and current.startswith(\"B\"):\n",
    "        answer=True\n",
    "    if prev.startswith(\"B\") and current.startswith(\"O\"):\n",
    "        answer=True\n",
    "    if prev.startswith(\"I\") and current.startswith(\"B\"):\n",
    "        answer=True\n",
    "    if prev.startswith(\"I\") and current.startswith(\"O\"):\n",
    "        answer=True\n",
    "    if prev!=\"O\" and current!=\"O\" and prev[2:]!=current[2:]:\n",
    "        answer=True\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def startofphrase(prev, current):  #if the current word is the first word of a NE phrase, then returns true\n",
    "    answer=False\n",
    "    if current.startswith(\"B\"):\n",
    "        answer=True\n",
    "    if prev.startswith(\"O\") and current.startswith(\"I\"):\n",
    "        answer=True\n",
    "    if prev!=\"O\" and current!=\"O\" and prev[2:]!=current[2:]:\n",
    "        answer=True\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_basic(predlist,truelist):\n",
    "    if len(predlist)!=len(truelist):\n",
    "        #sanity check\n",
    "        print(\"not same!!!\")\n",
    "        return None\n",
    "    total = len(predlist)\n",
    "    tp = 0\n",
    "    retrieved = 0\n",
    "    relevant = 0\n",
    "    correct=0\n",
    "\n",
    "    check=False #checks if the prediction matches the true tag. this turns true when both detects a start a phrase. It remains true until an error occurs, or if the phrase ends\n",
    "\n",
    "\n",
    "    if total>1:\n",
    "\n",
    "        #loop through the sentence. True positive, retrieved and relevant are all counted on a phrase-basis (one phrase is counted as 1)\n",
    "\n",
    "        for i in range(0,total):\n",
    "            if i==0:\n",
    "                trueprev='O'\n",
    "                predprev='O'\n",
    "            else:\n",
    "                trueprev = truelist[i - 1]\n",
    "                predprev = predlist[i - 1]\n",
    "\n",
    "\n",
    "            truecurrent=truelist[i]\n",
    "            predcurrent = predlist[i]\n",
    "\n",
    "\n",
    "            if startofphrase(trueprev,truecurrent)==True:\n",
    "                relevant+=1\n",
    "\n",
    "            if startofphrase(predprev,predcurrent)==True:\n",
    "                retrieved+=1\n",
    "\n",
    "            if check==True:\n",
    "                if endofphrase(trueprev,truecurrent) ==True and endofphrase(predprev,predcurrent)==True and trueprev[2:]==predprev[2:]:\n",
    "                    tp+=1\n",
    "                    check=False\n",
    "                if truecurrent[2:]!=predcurrent[2:] or endofphrase(trueprev,truecurrent)!=endofphrase(predprev,predcurrent):\n",
    "                    check=False\n",
    "\n",
    "            if startofphrase(trueprev, truecurrent) == True and startofphrase(predprev,predcurrent) == True and truecurrent[2:] == predcurrent[2:]:\n",
    "                        check = True\n",
    "                \n",
    "        if check==True: #this is to fill in the gap of the for-loop above. if the last word is in a NE and so far the check is ok , then this is also a true positive\n",
    "            tp+=1\n",
    "\n",
    "    elif total==1: #one-token sentence\n",
    "\n",
    "        if truelist[0] == predlist[0]:\n",
    "            correct += 1\n",
    "\n",
    "\n",
    "        if truelist[0] != \"O\":\n",
    "            relevant += 1\n",
    "\n",
    "        if predlist[0] != \"O\":\n",
    "            retrieved += 1\n",
    "        if truelist[0] == predlist[0] and truelist[0]!='O':\n",
    "            tp+=1\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #you can print it if you want, to inspect it\n",
    "    print(\"Total:\",total)\n",
    "    print(\"Releant:\" , relevant)\n",
    "    print(\"Retrieved:\" , retrieved)\n",
    "    print(\"TP:\" , tp)\n",
    "    \"\"\"\n",
    "\n",
    "    return relevant,retrieved,tp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline &  72.0 & 50.9 & 59.7 & 2875 & 3991 & 5648 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tp = 0\n",
    "retrieved = 0\n",
    "relevant= 0\n",
    "\n",
    "\n",
    "results=[]\n",
    "\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "\n",
    "    truelist=y_truth[i]\n",
    "    predlist=predictions[i]\n",
    "    relevant,retrieved,tp=performance_basic(predlist,truelist)\n",
    "\n",
    "    result=(relevant,retrieved,tp)\n",
    "\n",
    "\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "\n",
    "relevant = [x[0] for x in results]\n",
    "relevant = sum(relevant)\n",
    "\n",
    "retrieved = [x[1] for x in results]\n",
    "retrieved = sum(retrieved)\n",
    "\n",
    "tp = [x[2] for x in results]\n",
    "tp = sum(tp)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    precision = tp / retrieved\n",
    "except:\n",
    "    precision=0\n",
    "\n",
    "\n",
    "try:\n",
    "    recall = tp / relevant\n",
    "except:\n",
    "    recall=0\n",
    "\n",
    "try:\n",
    "    fscore = 2 * precision * recall / (precision + recall)\n",
    "except:\n",
    "    fscore=0\n",
    "\n",
    "\n",
    "precision=round(precision*100,1)\n",
    "recall=round(recall*100,1)\n",
    "fscore=round(fscore*100,1)\n",
    "\n",
    "\n",
    "\n",
    "#print(relevant)\n",
    "print(f\"Baseline &  {precision} & {recall} & {fscore} & {tp} & {retrieved} & {relevant} \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We have shown how to do NER using HMM. If trained and tested CoNLL-2003 dataset,\n",
    "it should have 72% precision, 50.9% recall and 59.7% F-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
